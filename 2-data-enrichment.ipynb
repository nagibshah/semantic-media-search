{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36a26325-496e-4259-9561-69fb5ae71a1a",
   "metadata": {},
   "source": [
    "# Multi Modal Semantic Search - Data Enrichment\n",
    "\n",
    "This notebook provides a walkthrough of a data enrichment of media assets using multi-modal LLMs. Raw images contain a lot of information and often these information are never leveraged during search. It is the purpose of this notebook to extract these metadata contained within the media assets such relevant descriptions, keywords and the tag. \n",
    "\n",
    "Often enrichment of media assets require a human to provide a description. Since the advent of Multi-modality of LLMs it is now possible to extract the relevant metadata from an asset in an automated fashion. This is not limited to media assets but can also be extended to graphic, illustrations, charts, graphs and other media formats. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30730738-4416-400d-afde-18abc99de0d0",
   "metadata": {},
   "source": [
    "The following cell loads all the required libraries including instantiating a boto3 bedrock client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d80edff-f969-471a-835b-d3cc6a6e9d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "import pprint as pp\n",
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "import time\n",
    "import pprint\n",
    "import base64\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#get bedrock instances with boto3\n",
    "bedrock = boto3.client('bedrock')\n",
    "bedrock_client = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a19044-a391-4d5b-91e3-4aea5d6d6129",
   "metadata": {},
   "source": [
    "Load the dataframe from previous notebook which includes real and fabricated images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220b82f5-e7b3-4097-b5fe-7597b12feab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset from notebook 1 \n",
    "%store -r df_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27e3969-c3e8-4f31-bc42-c896b7e825dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_images.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcf605a-aa73-4ab8-bcf7-e296c16a6893",
   "metadata": {},
   "source": [
    "## Enrichment from Raw Media assset using Bedrock \n",
    "\n",
    "For this step, we are going to leverage Claude 3 sonnet from Anthropic, which has multi-modal capabilities to extract various metadata from the raw media asset. \n",
    "\n",
    "Claude 3.5 Sonnet offers best-in-class vision capabilities compared to other leading models. It can accurately transcribe text from imperfect imagesâ€”a core capability for retail, logistics, and financial services, where AI may glean more insights from an image, graphic, or illustration than from text alone. The latest Claude models demonstrate a strong aptitude for understanding a wide range of visual formats, including photos, charts, graphs and technical diagrams. With Claude, you can extract more insights from documents, process web UI and diverse product documentation, generate image catalog metadata, and more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6715ac-d726-449c-b912-3bccc64dd80f",
   "metadata": {},
   "source": [
    "The following cell defines some helper functions to invoke Claude Sonnet 3.5 model as well as other handy utility functions to do convert images to base 64 as well as display images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e00ee0e-239a-43ca-a977-a19f2fa9f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions for generating metadata and creating some previews\n",
    "\n",
    "modelid = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "\n",
    "\n",
    "def image_to_base64(path):\n",
    "    # Read reference image from file and encode as base64 strings.\n",
    "    content_image = \"\"\n",
    "    with open(path, \"rb\") as image_file:\n",
    "        content_image = base64.b64encode(image_file.read()).decode('utf8')\n",
    "    return content_image\n",
    "\n",
    "def generate_metadata(bedrock_runtime, model_id, messages, max_tokens,top_p,temp):\n",
    "\n",
    "    body=json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temp,\n",
    "            \"top_p\": top_p\n",
    "        }  \n",
    "    )  \n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(body=body, modelId=model_id)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    response_text = response_body[\"content\"][0][\"text\"]    \n",
    "\n",
    "    return response_text\n",
    "\n",
    "def bulk_extract_metadata(df_images):\n",
    "    return df_images    \n",
    "\n",
    "def display_image(path):\n",
    "    im = Image.open(path)\n",
    "    plt.imshow(im)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72896fc2-ee7f-4a75-967d-7ec611d56dec",
   "metadata": {},
   "source": [
    "### Enrichment of single media asset\n",
    "\n",
    "Before running a batch operation accross all the assets, the following cells attempts to do enrichment for a single media asset for an initial test. For this purposes we are simply going to pick the first image in the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271f6f4d-600f-4a6b-afe9-72ccb4849ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first enrich a single image \n",
    "# view the image we are trying to process - randomly pick an item\n",
    "test_image_path = df_images.sample()[\"path\"].values[0]\n",
    "display_image(test_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b91627-ceff-441f-a5e6-1cb55469483f",
   "metadata": {},
   "source": [
    "The following cell defines Sonnet 3.5 prompt that we will be utilising to extract the required metadata. A basic COSTAR (context, objective, style, tone, audience, response) format is applied here. Note the use of XML tags which is an Anthropic specific prompt engineering best practice.\n",
    "\n",
    "Since Sonnet 3.5 is has extensive vision capabilities we are are also utilising the model to extract various OCR texts that may be contained within the image. It is important to note that it is possible to create domain specific (e.g. travel, CPG) prompts to extract relevant metadata. Prompt engineering should be done to meet specific business needs and requirements and tested for accuracy before production use. \n",
    "\n",
    "Further information on [prompt engineering](https://aws.amazon.com/blogs/machine-learning/prompt-engineering-techniques-and-best-practices-learn-by-doing-with-anthropics-claude-3-on-amazon-bedrock/) can be found in this useful blog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "276709a0-9d95-477f-8b57-51eac812f752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enrichment prompt \n",
    "# prepare the multimodal content message input for Claude 3 (separate json objects for image and text)\n",
    "# define a prompt to get a title, description, tags, keywords from Claude \n",
    "# follow the costar principle (context, objective, style, tone, audience, response) \n",
    "prompt = \"\"\"\n",
    "You are a multimodal search engine metadata extractor. Your task is to carefully study the image, and extract tags, keywords, description, any text contained in the image, as well as forming a succinct title for the image.\n",
    "Do not generated unwanted and uncessarily metadata, that will bloat the payload. Be detailed on various aspects of the image such as background, foreground and subject of the image.\n",
    "If media contains no OCR texts then simply generate an empty string for it. \n",
    "Make necessary determination on the key category/theme of the image and generate keywords as appropriate. \n",
    "\n",
    "All metadata extracted is required to be professional for official use. Refrain from using informal language. \n",
    "\n",
    "The intended audience are various business users who will attempt to search based on keywords or describing in natural language what they are looking for. For this purpose generate a description the image and include finer details that can be used for search purposes alongisde keywords and tags. \n",
    "\n",
    "Output the metadata as well formed JSON as defined in the <example> XML tags. Do not include XML tags in the output.  \n",
    "\n",
    "<example>\n",
    "    {\n",
    "        \"title\":\"baseball player swinging bat\",\n",
    "        \"description\":\"baseball player wearing team jersey swinging bat to score a home run. In the background there are cheering fans who are joyous and having a wonderful time. Stadium atmosphere seems electric.\",\n",
    "        \"keywords\":\"baseball, sports, homerun, bat, player, jersey\",\n",
    "        \"tags\":\"sports, stadium, baseball, crowd\"\n",
    "        \"ocr_texts\":\"new york mets, mets, new york, richardson, 66\"\n",
    "    }\n",
    "<example>\n",
    "\"\"\"\n",
    "\n",
    "def prepare_mm_input(path):\n",
    "    # Read reference image from file and encode as base64 strings.\n",
    "    content_image = image_to_base64(path)\n",
    "    message_mm = [\n",
    "        { \"role\": \"user\",\n",
    "          \"content\": [\n",
    "          {\"type\": \"image\",\"source\": { \"type\": \"base64\",\"media_type\":\"image/jpeg\",\"data\": content_image}},\n",
    "          {\"type\": \"text\",\"text\": prompt}\n",
    "          ]\n",
    "        }\n",
    "    ]\n",
    "    return message_mm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd5ef46-a898-4213-b35d-a9cae809f9bd",
   "metadata": {},
   "source": [
    "The following cells invokes the Sonnet model and prints the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4505bdf9-d46f-49cb-8912-a2fa07b75024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the single image \n",
    "# note: the enriched output quality will vary depending on quality of image (ie. blurry text) \n",
    "message_mm = prepare_mm_input(test_image_path)\n",
    "response = generate_metadata(bedrock_client, model_id = modelid,messages=message_mm,max_tokens=512,temp=0.5,top_p=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc356557-1f43-4329-bf71-fc1ff6c88478",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pp(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c35a22d-20fb-4c5d-99ed-cc1140539efc",
   "metadata": {},
   "source": [
    "### Bulk metadata extraction using Claude Sonnet \n",
    "\n",
    "Now that we can verify the enrichment for one image, we can apply the same logic accross all the media assets in the dataframe. Ideally, this process should be written and run in an enrichment pipeline but for simplicy we have opted to simply run it accross 5 threads and create a new dataframe with all LLM extracted keywords, description, title, tags, and OCR texts. \n",
    "\n",
    "Note: This process may take several minutes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94ca0e3-2b8b-4d95-a444-d74658794861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from multiprocessing.dummy import Pool as ThreadPool \n",
    "\n",
    "pool = ThreadPool(5) \n",
    "\n",
    "# called by each thread\n",
    "def get_metadata(item):\n",
    "    message_mm = prepare_mm_input(item[1])\n",
    "    response = generate_metadata(bedrock_client, model_id = modelid,messages=message_mm,max_tokens=512,temp=0.5,top_p=0.9)\n",
    "    json_response = json.loads(response)\n",
    "    response = {\n",
    "        \"image_id\":item[0],\n",
    "        \"path\":item[1],\n",
    "        \"title\":json_response[\"title\"],\n",
    "        \"description\":json_response[\"description\"],\n",
    "        \"keywords\":json_response[\"keywords\"],\n",
    "        \"tags\":json_response[\"tags\"]\n",
    "    }\n",
    "    return response\n",
    "\n",
    "\n",
    "list_images = df_images.values.tolist()\n",
    "start_time = time.time()\n",
    "results = pool.map(get_metadata, list_images)\n",
    "duration = time.time() - start_time\n",
    "print(f\"Processed {len(list_images)} in {duration} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026fd2f2-77da-44e6-baa8-197360cc2773",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata = pd.DataFrame(results)\n",
    "df_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f174cdea-7948-43cc-970f-e4ae423369cd",
   "metadata": {},
   "source": [
    "## Generate Vector Embeddings\n",
    "\n",
    "A standard approach to conducting semantic and similarity search is to leverage vector embeddings created using an LLM capable of generating embeddings. \n",
    "\n",
    "Let's start with the basics: what is an embedding? An embedding is a numerical representation of content in a form that machines can process and understand. The essence of the process is to convert an object, such as an image or text, into a vector that encapsulates its semantic content while discarding irrelevant details as much as possible. An embedding takes a piece of content, like a word, sentence, or image, and maps it into a multi-dimensional vector space. The distance between two embeddings indicates the semantic similarity between the corresponding concepts.\n",
    "\n",
    "Consider the terms 'coffee' and 'tea'. In a hypothetical vocabulary space, these two could be transformed into numerical vectors. If we visualize this in a 3-dimensional vector space, 'coffee' might be represented as [1.2, -0.9, 0.3] and 'tea' as [1.0, -0.8, 0.5]. Such numerical vectors carry semantic information, indicating that 'coffee' and 'tea' are conceptually similar to each other due to their association with hot beverages and would likely be positioned closer together in the vector space than either would be to unrelated concepts like 'astronomy' or 'philosophy'.\n",
    "\n",
    "For the purpose of this exercise we are going to utilise Titan Multimodal Embeddings G1 model. The Titan Multimodal Embeddings G1 model translates text inputs (words, phrases or possibly large units of text) as well as images into numerical representations (known as embeddings) that contain the semantic meaning of the text and image. While this model will not generate text as an output, it is useful for applications like personalization and search.\n",
    "\n",
    "In later notebooks, we will be storing these embeddings in a vector database to conduct search operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18ffc16-63c1-41ac-9536-357b6b550c21",
   "metadata": {},
   "source": [
    "The following cell defines a function to generate the embeddings by invoking titan-embed-image-v1 model. Since the input payload may also be an image asset we are required to create base64 encoding of the image before carrying out the model invocation. Note the output vector dimension of 1024 default but smaller embeddings space can be created if needed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6998deb-9bc7-441a-b40d-44cc98f4d712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to generate embeddings\n",
    "\n",
    "# Select Amazon titan-embed-image-v1 as Embedding model for multimodal indexing\n",
    "multimodal_embed_model = f'amazon.titan-embed-image-v1'\n",
    "\n",
    "def generate_embeddings(\n",
    "    imgpath: str = None,  # maximum 2048 x 2048 pixels\n",
    "    text: str = None,  # optional text to embed\n",
    "    dimension: int = 1024,  # 1,024 (default), 384, 256\n",
    "    model_id: str = multimodal_embed_model\n",
    "):\n",
    "    payload_body = {}\n",
    "    embedding_config = {\n",
    "        \"embeddingConfig\": { \n",
    "             \"outputEmbeddingLength\": dimension\n",
    "         }\n",
    "    }\n",
    "\n",
    "    # titan can generate embeddings for txt and img \n",
    "    if imgpath:\n",
    "        with open(imgpath, \"rb\") as image_file:\n",
    "            input_image = base64.b64encode(image_file.read()).decode('utf8')\n",
    "            payload_body[\"inputImage\"] = input_image\n",
    "    if text:\n",
    "        payload_body[\"inputText\"] = text\n",
    "\n",
    "    response = bedrock_client.invoke_model(\n",
    "        body=json.dumps({**payload_body, **embedding_config}),\n",
    "        modelId=model_id,\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    return json.loads(response.get(\"body\").read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c3200-7ce5-45d6-8fb5-9386c1263545",
   "metadata": {},
   "source": [
    "Following cell generates embedding for the same test image we picked for enrichment purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ead0d6-b6e6-4dcd-9e83-314052dd2160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets generate the embeddings for the same test image\n",
    "\n",
    "embedding = generate_embeddings(test_image_path)[\"embedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18952e2-9c5d-4ce0-8382-a4a40946b557",
   "metadata": {},
   "source": [
    "Now that we can generate embeddings for a single image, we are required to batch process the dataframe and generate embeddings for all images in the dataframe. A simple iterator is used to loop over the dataset and adding the record to the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1fa44c-b68b-42e7-863f-877890de2630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the embeddings for all the images in the dataset \n",
    "multimodal_embeddings_img = []\n",
    "\n",
    "for idx, image in df_metadata.iterrows():\n",
    "    embedding = generate_embeddings(imgpath=image['path'])[\"embedding\"]\n",
    "    multimodal_embeddings_img.append(embedding)\n",
    "\n",
    "df_metadata = df_metadata.assign(embeddings=multimodal_embeddings_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b68d042-3ab9-4dae-992e-b3b6f7d13310",
   "metadata": {},
   "source": [
    "Quick preview of the dataframe which should now contain all enriched data extracted using Claude Sonnet as well as the emebddings generated using Titan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3541fc-0717-4785-ae55-c72e35516e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets preview the dataset which should now contain a fully enriched dataset\n",
    "df_metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4b62d2-ea6a-4f83-9338-902aeea1185f",
   "metadata": {},
   "source": [
    "The following cell simply loads a custom helpers module from the ./src/helpers directory to the system path. This module contains various helper functions that will be used throughout the labs. For now we only require the similary heat map function to create a heatmap of all the embeddings contained in the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0807c388-7b1e-4d8d-a48f-32c541cf78ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding our utils library to sys path\n",
    "import sys\n",
    "sys.path.append(\"./src/helpers/\")\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711f2c55-22d0-48d4-9317-3031964bc478",
   "metadata": {},
   "source": [
    "The heatmap below should indicate a clear pattern since we have overloaded the dataset with some fabricated images that are similar in nature. This is done for illustrative purposes only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb91b94-9ea6-43ef-b22d-d8aeadbe2c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a similarity heatmap of the images in the dataset based on the embeddings \n",
    "# the heatmap should theoretically indicate distinct patches for baseball, savannah, lion due to addition of fabiracated images\n",
    "plot_similarity_heatmap(multimodal_embeddings_img, multimodal_embeddings_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54899cba-7de1-4967-9d85-fb10ae52f67e",
   "metadata": {},
   "source": [
    "Following cells saves the enriched dataframe as well as writing the dataframe as a CSV file to disc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85096096-9320-48ad-8b1a-05180c341026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save for use in next notebook \n",
    "%store df_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10cba1d-a58c-49b1-bb58-c0d921b02960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as CSV for later use \n",
    "df_metadata.to_csv('./data/enriched_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f95c1-a8ef-4ed3-a59a-e244c96909fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
