{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbfbe789-de5b-43df-bb73-ed8828bfcd88",
   "metadata": {},
   "source": [
    "# Multi-Modal Semantic, Lexical and Hybrid Search "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1e373a-8f75-46d2-b9b4-4f5db60e9039",
   "metadata": {},
   "source": [
    "The following cell installs required python libraries specified in the 'requirements.txt' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552adf64-d12c-4644-8595-b36644fc19ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load required libraries \n",
    "import os\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "import pprint\n",
    "import random \n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth, helpers\n",
    "import time\n",
    "\n",
    "session = boto3.session.Session()\n",
    "region_name = session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1766ec54-3a66-44e4-b1e1-c5564d78831b",
   "metadata": {},
   "source": [
    "Load the utility library from the src folder which contains various functions for this notebook ease of use and simplicy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aae707-37b6-456e-8e56-7cf3866f3221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#adding our utils library to sys path\n",
    "import sys\n",
    "sys.path.append(\"./src/helpers/\")\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78295aaf-374c-4f09-850f-98e25f7f46b5",
   "metadata": {},
   "source": [
    "Before we begin conducting search let us reload the required datasets and variables from previous notebooks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fcdcaf-e1cc-49d4-b380-4a60aa6d2d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the dataset from notebook 2 \n",
    "%store -r df_metadata\n",
    "# df_metadata = pd.read_csv('./data/enriched_dataset.csv')\n",
    "df_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ce1dc3-af62-4078-8b84-ad9e940ce25e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retrieve the index name and collection details for OSS from previous notebook\n",
    "%store -r index_name\n",
    "%store -r collection_name\n",
    "%store -r host\n",
    "\n",
    "print(\"opensearch host:{0}, index:{1}, collection:{2}\".format(host, index_name, collection_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b6d9a7-63b1-4bd2-bc82-bec4efe329ea",
   "metadata": {},
   "source": [
    "## Preparing the OSS Client\n",
    "\n",
    "This section demonstrates how to search the index using vector embeddings and the k-nearest neighbors (KNN) algorithm as well as conduct classic lexical search in OSS. It generates an embedding for a given query text as well as a reference image, constructs a KNN query, and retrieves the top k most relevant documents from the index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aced6fdf-6523-49a3-9bbb-9f3e7636b4f3",
   "metadata": {},
   "source": [
    "Before we can query, we first need to build a client using the opensearch library. The following cell builds a client using the current users credentials as well as the host name we saved in previous notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5db1e65-020a-40ea-b4e3-52992157556b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the collection of type vector search\n",
    "oss_client = boto3.client('opensearchserverless')\n",
    "# create the OSS client\n",
    "service = 'aoss'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWSV4SignerAuth(credentials, region_name, service)\n",
    "\n",
    "\n",
    "# Build the OpenSearch client\n",
    "search_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a847774-4ae2-4c0b-aa25-d4152b79fe18",
   "metadata": {},
   "source": [
    "## Search the index \n",
    "\n",
    "In this section, we will be conducting various styles of searches with opensearch from classic Lexical search using keywords and query matching, to more advanced vector search using the KNN algorithm as well as hybrid searches. \n",
    "\n",
    "Nearest neighbor search is conducted by simply measuring the distance between vectors. There are different distance metrics used in vector similarity calculations such as:\n",
    "Euclidean distance: It measures the straight-line distance between two vectors in a vector space. It ranges from 0 to infinity, where 0 represents identical vectors, and larger values represent increasingly dissimilar vectors.\n",
    "\n",
    "Cosine distance: This similarity measure calculates the cosine of the angle between two vectors in a vector space. It ranges from -1 to 1, where 1 represents identical vectors, 0 represents orthogonal vectors, and -1 represents vectors that are diametrically opposed.\n",
    "\n",
    "Dot product: This measure reflects the product of the magnitudes of two vectors and the cosine of the angle between them. Its range extends from -∞ to ∞, with a positive value indicating vectors that point in the same direction, 0 indicating orthogonal vectors, and a negative value indicating vectors that point in opposite directions.\n",
    "\n",
    "![Alt text](./static/distance.png \"Distance metrics\")\n",
    "\n",
    "k-NN or rather approximate Nearest Neighbor (ANN) is the foundation for conducting similary search in OpenSearch and it is supported by nmslib, faiss, and Lucene libraries. Each of the three engines used for approximate k-NN search has its own attributes that make one more sensible to use than the others in a given situation. You can follow the general information below to help determine which engine will best meet your requirements.\n",
    "\n",
    "k-NN for Amazon OpenSearch Service lets you search for points in a vector space and find the \"nearest neighbors\" for those points by Euclidean distance or cosine similarity. \n",
    "\n",
    "In general, nmslib outperforms both faiss and Lucene on search. However, to optimize for indexing throughput, faiss is a good option. For relatively smaller datasets (up to a few million vectors), the Lucene engine demonstrates better latencies and recall. At the same time, the size of the index is smallest compared to the other engines, which allows it to use smaller AWS instances for data nodes.\n",
    "\n",
    "Overall, for larger data sets, you should generally choose the approximate nearest neighbor method because it scales significantly better.\n",
    "\n",
    "The purpose is to provide a clear indication of the varying output when text vector are used over direct image embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790f5a7c-9b64-4011-bc2d-7ae10d33697a",
   "metadata": {},
   "source": [
    "### OSS DSL queries\n",
    "The code cell below builds a opensearch DSL queries payload before calling opensearch via the search client. \n",
    "\n",
    "Two functions are defined here to conduct lexical as well as embeddings search using the KNN algorithm in opensearch. For the similarity search using embeddings it requires an additoinal k parameter to find k number of nearest neighbors from the vector space. In literal terms this will find k number of similar items that are closest to the input emebeddings in euclidean distance. Opensearch directly does this evaluation therefore we only need to specify the number of neighbors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861a888b-2abf-46f2-a057-60bcd3455c68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper functions to conduct lexical, vector similarity, as well as hybrid style vector and filter search\n",
    "\n",
    "def query_text_description(search_term, hits=10):\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"title\": {\n",
    "                    \"query\": search_term\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"_source\": {\n",
    "            \"exclude\": [\"image_vector\"], # exclude vectors from the response\n",
    "        },\n",
    "        \"size\": hits\n",
    "    }\n",
    "    \n",
    "    search_response = search_client.search(body=query, index=index_name)\n",
    "    return search_response\n",
    "\n",
    "def query_similarity(query_emb, hits=10, k=5):\n",
    "    \n",
    "    body = {\n",
    "            \"query\": {\n",
    "                \"knn\": {\n",
    "                    \"image_vector\": {\n",
    "                        \"vector\": query_emb,\n",
    "                        \"k\": k,\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            \"_source\": {\n",
    "                \"exclude\": [\"image_vector\"],\n",
    "            },\n",
    "            \"size\": hits\n",
    "    }     \n",
    "\n",
    "    search_response = search_client.search(index=index_name, body=body)\n",
    "    return search_response\n",
    "\n",
    "def query_similarity_filter(query_emb, filter_text, hits=10, k=5):\n",
    "    body = {\n",
    "            \"query\": {\n",
    "                \"knn\": {\n",
    "                    \"image_vector\": {\n",
    "                        \"vector\": query_emb,\n",
    "                        \"k\": k,\n",
    "                        \"filter\": {\n",
    "                            \"bool\": {\n",
    "                                \"must\": [\n",
    "                                    {\n",
    "                                        \"multi_match\": {\n",
    "                                          \"query\": filter_text,\n",
    "                                          \"fields\": [\n",
    "                                            \"title\", \"keywords\", \"tags\", \"description\"\n",
    "                                          ]\n",
    "                                        }\n",
    "                                      }\n",
    "                                ]\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            #\"post_filter\": {\n",
    "            #    \"match\": {\n",
    "            #        \"description\": \"rugby player wearing red shirt\"\n",
    "            #    }\n",
    "            #},\n",
    "            \"_source\": {\n",
    "                \"exclude\": [\"image_vector\"],\n",
    "            },\n",
    "            \"size\": hits\n",
    "    }\n",
    "\n",
    "    search_response = search_client.search(index=index_name, body=body)\n",
    "    return search_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749ef62f-3300-42e9-8eef-1c89e886348b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  set the search term \n",
    "search_term = \"baseball player swinging bat\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b14b75a-d604-4460-85b6-f084f9b8445f",
   "metadata": {},
   "source": [
    "### Lexical search "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02e6cfb-3cec-4f21-a03b-ba185cc439fa",
   "metadata": {},
   "source": [
    "The cell below utilises a utility function to display results as images from the util library by mapping the file name in our dataset and locating the path. This is done to keep the notebook less verbose. For subsequent scenarios where image previous are required from search results we are going to be utilising the same utility function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e6ef61-cf2c-4109-9587-9a2aada674e1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_lexical = query_text_description(search_term, 5)\n",
    "display_results(results_lexical, df_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a5bf0f-b444-4928-9714-a3227476acf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reload \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcb9c84-6da1-48b4-a932-c583644e43b8",
   "metadata": {},
   "source": [
    "### Semantic search using text embeddings\n",
    "\n",
    "In this section we will conduct the same search but this time utilise the titan text embeddings to convert the search term into vector embeddings and conduct a vector search using the KNN algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c80c491-61a2-41a0-a3b7-a289c8e6e771",
   "metadata": {},
   "source": [
    "Since the query term needs to be converted to a vector first, we will call a method in the utility library which in turn calls titan multi-modal embedding model to generate the query embeddings.\n",
    "\n",
    "Note: For this search to be functional we are required to call the same embeddings model that is used for the image embeddings task. In data enrichment notebook, we utilised Titan Multimodal Embeddings G1 model and will be using the same for query embeddings. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79d6100-ac4a-4313-9dec-306bfc1ce997",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_emb = get_titan_multimodal_embedding(description=search_term, dimension=1024)[\"embedding\"]\n",
    "results_semantic_text = query_similarity(query_emb, hits=5, k=2)\n",
    "display_results(results_semantic_text, df_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0eda91-2c32-4be2-a5f2-28208ada94d1",
   "metadata": {},
   "source": [
    "### Semantic search using Image embeddings\n",
    "\n",
    "In order for the search to take into account finder semantic meanings we can utilise a reference image as well as text for query embeddings. This will allow specific properties of hte image to be embedded into the vector space therefore return a tighter result set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f057f7-5935-445d-90d3-ad424929d8c0",
   "metadata": {},
   "source": [
    "The following cell generates a base image using titan image generator for the purpose of search \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cfa28c-10ed-48bc-991c-b0bcb07cc2aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"ultra photorealistic professional baseball player blue jersey about to hit a home run, ballfield, spectators\"\n",
    "neg_prompt = \"blurry, low quality, distorted\"\n",
    "test_img = generate_titan_image(prompt=prompt,neg_prompt=neg_prompt, seed=random.randint(1,9))\n",
    "path = \"./data/testimg.png\"\n",
    "test_img.save(path)\n",
    "test_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ba3050-37c9-4e84-a5b2-89fa78af3f23",
   "metadata": {},
   "source": [
    "Use the test image as reference image to now conduct a similarity search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1523d7bd-ba73-417a-8303-18975519f354",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_emb = get_titan_multimodal_embedding(image_path=path, dimension=1024)[\"embedding\"]\n",
    "results_semantic_img = query_similarity(query_emb, hits=5, k=2)\n",
    "display_results(results_semantic_img, df_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b656eb-3827-4dc0-8216-49901d00cf7b",
   "metadata": {},
   "source": [
    "Since titan embedding can take in both image and text, the following cell combines a user supplied term as well as a reference image. We are again utilising a util function to get a single embedding from the image and text. The embedding is them used to carry out a knn search. For this particular instance we are attempting to find a similar image but the action is \"diving to catch the ball\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dac77f-59e1-403f-9730-dd49ecc19b9e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "optional_description = \"diving catch\"\n",
    "query_emb = get_titan_multimodal_embedding(image_path=path, description=optional_description, dimension=1024)[\"embedding\"]\n",
    "results_semantic_img_txt = query_similarity(query_emb, hits=10, k=2)\n",
    "display_results(results_semantic_img_txt, df_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aa55bb-463d-4d99-b78d-a082a870e6e6",
   "metadata": {},
   "source": [
    "### Semantic search using generated description\n",
    "\n",
    "For this step, we are going to extract text description of the image using Claude Sonnet which in turn will be used to carry out embeddings search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f660bd78-d2b7-425e-9829-b52e11eb72b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You are a multimodal search engine metadata extractor. Your task is to carefully study the image and generate a text description of the image.\n",
    "Be detailed on various aspects of the image such as background, foreground, subject, and other finer details of the image.\n",
    "\n",
    "Make necessary determination on the key category/theme of the image and generate description as appropriate. \n",
    "\n",
    "Description generated is required to be professional for official use. Refrain from using informal language. \n",
    "\n",
    "The intended audience are various business users who will attempt to search based on a description in natural language of what they are looking for.  \n",
    "\n",
    "Output the description as plain text only. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a9ec45-d216-426c-8c2c-ed38981eea37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res=generate_img_desc(prompt=prompt, img_path=path)\n",
    "pprint.pp(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef9d7e-c959-4079-b3e9-94db779e91d1",
   "metadata": {},
   "source": [
    "use the description of the image as the source text embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04f601a-b864-4ab9-a5ce-58eb31a2406c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "optional_description = \"diving catch\" # this can be user supplied text in addition to the image\n",
    "search_term = res + optional_description\n",
    "query_emb = get_titan_multimodal_embedding(description=search_term, dimension=1024)[\"embedding\"]\n",
    "results_semantic_gen_text = query_similarity(query_emb, hits=10, k=2)\n",
    "display_results(results_semantic_gen_text, df_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c8fbcb-0461-48b4-b6af-e3cf18d5e90e",
   "metadata": {},
   "source": [
    "## Hybrid Search - the best of both worlds? \n",
    "\n",
    "Hybrid search is a technique that combines traditional lexical search and semantic search to improve search relevance.\n",
    "\n",
    "The motivation behind this approach is that different queries perform better with either lexical or semantic search. By combining these two methods, search performance can be enhanced. The challenge in implementing hybrid search lies in normalizing the similarity scores produced by the two types of search, as they use different scales.\n",
    "\n",
    "\n",
    "Various benchmarks show a significant performance improvement using Hybrid search versus lexical or semantic only. Example: https://opensearch.org/blog/hybrid-search/\n",
    "\n",
    "\"hybrid search improves the result quality by 8–12% compared to keyword search and by 15% compared to natural language search\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8f138a-c393-4d78-87c4-b4049c24f3f6",
   "metadata": {},
   "source": [
    "### Hybrid style search with knn filters\n",
    "\n",
    "Opensearch includes efficient query filtering with k-NN FAISS engine. OpenSearch’s efficient vector query filters capability intelligently evaluates optimal filtering strategies—like pre-filtering with approximate nearest neighbor (ANN) or filtering with exact k-nearest neighbor (k-NN)—to determine the best strategy to deliver accurate and low latency vector search queries.\n",
    "\n",
    "Vector query filters is a key enabler for hybrid search empowering users to perform vector search while filtering on metadata to retrieve more relevant information using both vector and lexical techniques.\n",
    "\n",
    "The following cell runs a DSL query that includes a k-NN filter to refine the results. We can utilise other clauses to make this query more refined and powerful but for this instance the DSL query simply does a multi-match on title, keyword, and tags field based on the user supplied string as well as the reference image profided. Review the query_similarity_filter method to see how this DSL is constructed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fa7aba-f9c4-444b-b8c7-7254800db204",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#optional_description = \"celebrate victory\" # this can be user supplied text in addition to the image\n",
    "#optional_description = \"sliding into base\"\n",
    "optional_description = \"diving catch\"\n",
    "query_emb = get_titan_multimodal_embedding(image_path=path, dimension=1024)[\"embedding\"]\n",
    "results_semantic_img_filter = query_similarity_filter(query_emb,filter_text=optional_description, hits=10, k=2)\n",
    "display_results(results_semantic_img_filter, df_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bca813-f042-48ba-a45f-36555d4868e1",
   "metadata": {},
   "source": [
    "### Hybrid search using Rank Fusion\n",
    "\n",
    "With non serverless deployment of Amazon OpenSearch service using features released in September 2023 as part of OpenSearch 2.10, you can do that relatively easily using a \"search pipeline\" that includes a normalization processor. The pipeline runs at search time and normalizes the scores to a common scale, enabling meaningful combination. The process involves using min-max normalization and harmonic mean as the normalization and combination techniques, respectively. Additionally, the weights assigned to the query clauses can be adjusted to balance the influence of lexical and semantic search.\n",
    "\n",
    "Amazon OpenSearch Serverless does not support this feature yet so you need to develop the feature outside of Opensearch for the moment.\n",
    "\n",
    "Same for other vectorDBs including Amazon RDS for PostgreSQL using pgvector for example.\n",
    "\n",
    "\n",
    "#### RFF - Reciprocal Rank Fusion\n",
    "There are various strategies and algorithm to merge 2 lists of results.\n",
    "\n",
    "The Reciprocal Rank Fusion (RRF) is a sophisticated algorithm that combines different sets of results, each with its own relevance scores, into a single unified set of results. One of the main benefits of RRF is that it can produce high-quality results without requiring any adjustments or tuning. Additionally, RRF does not require the relevance scores from different sources to be related or similar in nature.\n",
    "\n",
    "How does it work?\n",
    "\n",
    "The Reciprocal Rank Fusion (RRF) technique works by gathering search results from multiple different approaches, assigning each document in the results a score based on its reciprocal rank, and then combining these scores to produce a new ranking. The core idea behind this method is that documents that consistently appear at higher ranks across various search strategies are more likely to be relevant, and therefore, should receive a higher ranking in the consolidated search result.\n",
    "\n",
    "Algorithm explained here: https://www.elastic.co/guide/en/elasticsearch/reference/current/rrf.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424fdaa5-81de-4aa8-94bb-79daf5198483",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RFF method \n",
    "\n",
    "#merge 2 lists of movies and remove duplicates\n",
    "def merge_remove_duplicates(list1, list2):\n",
    "    # Create an empty dictionary to store unique entries\n",
    "    unique_entries = {}\n",
    "\n",
    "    # Iterate over the first list\n",
    "    for entry in list1:\n",
    "        image_id = entry['image_id']\n",
    "        unique_entries[image_id] = entry\n",
    "\n",
    "    # Iterate over the second list\n",
    "    for entry in list2:\n",
    "        image_id = entry['image_id']\n",
    "        if image_id not in unique_entries:\n",
    "            unique_entries[image_id] = entry\n",
    "\n",
    "    # Convert the dictionary back to a list\n",
    "    merged_list = list(unique_entries.values())\n",
    "\n",
    "    return merged_list\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(list1, list2):\n",
    "    # Create a dictionary to store ranks\n",
    "    ranks = {}\n",
    "\n",
    "    max_size = max(len(list1), len(list2))\n",
    "    \n",
    "    # Assign ranks to items in list1\n",
    "    for rank, item in enumerate(list1, start=1):\n",
    "        image_id = item['image_id']\n",
    "        ranks.setdefault(image_id, []).append(rank)\n",
    "    \n",
    "    # Assign ranks to items in list2\n",
    "    for rank, item in enumerate(list2, start=1):\n",
    "        image_id = item['image_id']\n",
    "        ranks.setdefault(image_id, []).append(rank)\n",
    "    \n",
    "    #ranks will look like: {'168259': [1, 5], '460846': [2, 12], '294254': [3]}\n",
    "    \n",
    "    # Calculate RRF scores\n",
    "    # example: for '168259': [1, 5], the RFF score is 1/1 + 1/5 = 1.2\n",
    "    rrf_scores = {}\n",
    "    for image_id, ranks_list in ranks.items():\n",
    "        rrf_score = sum(1 / rank for rank in ranks_list)\n",
    "        rrf_scores[image_id] = rrf_score\n",
    "    \n",
    "    merged_list = merge_remove_duplicates(list1, list2)\n",
    "    for item in merged_list:\n",
    "        image_id = item['image_id']\n",
    "        item['RFF_score'] = rrf_scores.get(image_id, 0)\n",
    "\n",
    "    sorted_merged_list = sorted(merged_list, key=lambda x: x['RFF_score'], reverse=True)\n",
    "\n",
    "    return sorted_merged_list[:max_size]\n",
    "\n",
    "def format_rrf_search_result(res):\n",
    "    formatted_results = []\n",
    "    \n",
    "    for hit in res[\"hits\"][\"hits\"]:\n",
    "        item = {\n",
    "            \"image_id\":hit[\"_source\"][\"image_id\"],\n",
    "            \"path\":hit[\"_source\"][\"image_url\"],\n",
    "            \"title\":hit[\"_source\"][\"title\"]\n",
    "        }\n",
    "        formatted_results.append(item)\n",
    "    \n",
    "    return formatted_results\n",
    "\n",
    "def display_rrf_results(results, dataset):\n",
    "    images = []\n",
    "    for item in results:\n",
    "        image_id = item[\"image_id\"]\n",
    "        image, item_name = get_image_from_item_id(image_id = image_id, dataset = dataset,image_path = None)\n",
    "        image.name_and_score = \"{} : {}\".format(item[\"RFF_score\"],item[\"title\"])\n",
    "        images.append(image)\n",
    "    display_images(images)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366e85df-3474-49ea-8383-cf6e66ebb841",
   "metadata": {},
   "source": [
    "The following cell runs a lexical search and a semantic search using a reference image as well as the search term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ad4e1c-159c-4467-be45-76f4b3c178f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  set the search term \n",
    "search_term = \"diving to catch the ball\"\n",
    "\n",
    "# lexical search\n",
    "results_lexical = query_text_description(search_term, 5)\n",
    "# semantic search \n",
    "query_emb = get_titan_multimodal_embedding(image_path=path, description=search_term, dimension=1024)[\"embedding\"]\n",
    "#query_emb = get_titan_multimodal_embedding(description=search_term, dimension=1024)[\"embedding\"]\n",
    "results_semantic_text = query_similarity(query_emb, hits=5, k=2)\n",
    "results_semantic_img_txt = query_similarity(query_emb, hits=10, k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bf721e-f779-45ef-a257-d18ff09b96cf",
   "metadata": {},
   "source": [
    "Combining the two results using Reciprocal rank fusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaac7352-0665-4bd8-8409-a8fdf5d61d5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#---------- Hybrid search fusion with RRF -----------\n",
    "RFF_response  = reciprocal_rank_fusion(format_rrf_search_result(results_lexical), format_rrf_search_result(results_semantic_img_txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b91300-b7fa-40de-8688-bd2236fe5d94",
   "metadata": {},
   "source": [
    "Display the result of the RRF algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca94985-e2c5-40df-96a8-51ed9725067f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_rrf_results(RFF_response, df_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4007e59-0e7a-4655-abcf-fb82ca714ca7",
   "metadata": {},
   "source": [
    "# Advanced RAG with fusion and decomposition \n",
    "\n",
    "Fusion in RAG presents an innovative search strategy designed to transcend the limitations of conventional search techniques, aligning more closely with the complex nature of human inquiries. This initiative elevates the search experience by integrating multi-faceted query generation and using Reciprocal Rank Fusion for an enhanced re-ranking of search outcomes. This approach offers a more nuanced and effective way to navigate the vast expanse of available information, catering to the intricate and varied demands of users’ searches.\n",
    "\n",
    "The following diagram illustrates this flow. \n",
    "\n",
    "![Alt text](./static/mmrag-query-ranking.png \"Adanced Rag with Fusion and Decomposition\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a7d7a0-f1bb-4d79-8af7-28d1db284b61",
   "metadata": {},
   "source": [
    "## Evaluation of the search results\n",
    "\n",
    "Evaluation of search results requires ground truth results. In real life world we would have a search term and associated best results in the right ranking order. Since this is a lab and we have no set results, we are going to utilise an LLM as a judge to reorder images for a given search term as best as possible based on its reasoning capabilities. \n",
    "\n",
    "Since Claude Sonnet 3.5 has state of the art reasoning capabilities, we are going to define a prompt to reorder a static list of media assets based on its text description (which we generated utilising Sonnet in data prep lab). Due to the non deterministic nature of LLMs the final order of the ground truth dataset generated may vary from run to run. \n",
    "\n",
    "The rearranged list of media assets returned will used as ground truth for subsquent evaluation algorithms in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adef9b84-e473-4f39-9585-8eee78f86ceb",
   "metadata": {},
   "source": [
    "The following cell defines some helper functions to carry out dataframe to json transformation to be passed as context to the Claude. Additionally, we also have a list of search terms that we will be utilising for evaluation purposes. In this example we have 3 different search terms all related to baseball. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158f20e3-bc5f-4663-96f9-cc1293499f26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper functions \n",
    "\n",
    "search_terms_eval_list = [\n",
    "    \"diving to catch the ball\",\n",
    "    \"sliding into base\",\n",
    "    \"celebrating victory\"\n",
    "]\n",
    "\n",
    "#model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "\n",
    "def df_to_json(df):\n",
    "    json_list = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        json_dict = row.to_dict()\n",
    "        json_list.append(json_dict)\n",
    "    \n",
    "    return json_list\n",
    "\n",
    "def rearrange_list(bedrock_client, model_id, prompt, max_tokens,top_p,temp):\n",
    "    message_mm = [\n",
    "        { \"role\": \"user\",\n",
    "          \"content\": [\n",
    "              {\"type\": \"text\",\"text\": prompt}\n",
    "          ]\n",
    "        }\n",
    "    ]\n",
    "    body=json.dumps(\n",
    "        {\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"messages\": message_mm,\n",
    "            \"temperature\": temp,\n",
    "            \"top_p\": top_p\n",
    "        }  \n",
    "    )  \n",
    "    \n",
    "    response = bedrock_client.invoke_model(body=body, modelId=model_id)\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "    response_text = response_body[\"content\"][0][\"text\"]\n",
    "    return response_text\n",
    "\n",
    "def format_search_result(res):\n",
    "    formatted_results = []\n",
    "    \n",
    "    for hit in res[\"hits\"][\"hits\"]:\n",
    "        item = {\n",
    "            \"image_id\":hit[\"_source\"][\"image_id\"],\n",
    "            \"path\":hit[\"_source\"][\"image_url\"],\n",
    "            \"title\":hit[\"_source\"][\"title\"]\n",
    "        }\n",
    "        formatted_results.append(item)\n",
    "    \n",
    "    df = pd.DataFrame(formatted_results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e44e1d-59dd-48ca-a387-94e496faf9d9",
   "metadata": {},
   "source": [
    "The following cell filters all baseball related images from the dataframe based on the keyword field. This is done to work with a base list of all baseball images which we can reorder using another term (e.g. catching, diving). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc622dd9-a1b2-4752-a05e-05619848c350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first filter to get all image descriptions the contain baseball\n",
    "df_subset = df_metadata[df_metadata[\"keywords\"].str.contains(\"baseball\")]\n",
    "print(\"found {0} number of images post filter\".format(df_subset.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c40d581-c278-4166-86a8-058dc63e523a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_subset = df_subset.drop(columns=[\"embeddings\"]) # drop the embeddings\n",
    "df_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26da074e-7f04-4761-b200-c7138598f478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preview the set of images found to verify visually \n",
    "images = []\n",
    "for idx, item in df_subset.sample(5).iterrows():\n",
    "    img = Image.open(item['path'])\n",
    "    images.append(img)\n",
    "\n",
    "display_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29513134-8929-459b-98f3-f06152133aaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get Claude to rearrange and reorder the list - pushing photos with actual diving catch to higher rank\n",
    "prompt = \"\"\"\n",
    "Your task is to rearrange and rank a json payload containing a list of various images alongside their relevant metadata  such as image_id, path, title, description, keywords, and tags per image. \n",
    "The input list is provided in the <list> xml tags.\n",
    "The image_id field in the list contains the filename for the image. \n",
    "\n",
    "The rearranging and ranking must be done based on topic contained in the <topic> xml tags by carefully reviewing the topic against provided metadata and ascertain relevance. \n",
    "first item in the list should be the most relevant to the topic, then second and so forth. \n",
    "\n",
    "<topic>{term}</topic>\n",
    "\n",
    "<list> \n",
    "{images} \n",
    "</list> \n",
    "\n",
    "Important: Rearrange the list provided in the <list> XML tag only and do not fabricate any new data and do not exclude any images in the output. \n",
    "Return a valid json array in the output as specified in the <example> xml tag. Only image_id, path and title fields are required in the output. \n",
    "Do not include XML tags in the output. \n",
    "return only {k} number of items in the outout\n",
    "\n",
    "<example>\n",
    "[{\"image_id\":\"1efc2db85591a04f.jpg\",\"path\":\"./data/curated-images/1efc2db85591a04f.jpg\",\"title\":\"Baseball batter at home plate\"}]\n",
    "</example>\n",
    "\n",
    "[\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106be646-8337-4f61-8bc1-298229ac2a05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get a rearranged list using Claude as the judge based on the description for each image.\n",
    "subset_json = df_to_json(df_subset)\n",
    "formated_system_prompt = prompt.replace(\"{term}\", search_terms_eval_list[0]).replace(\"{images}\", str(subset_json)).replace(\"{k}\",str(10))\n",
    "rearranged_list = rearrange_list(bedrock_client, model_id = model_id, prompt=formated_system_prompt, max_tokens=4000, temp=0.1, top_p=0.9)\n",
    "df_rearranged = pd.DataFrame(json.loads(rearranged_list))\n",
    "df_rearranged.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b10f665-6373-4165-b431-975df456e0cc",
   "metadata": {},
   "source": [
    "#### Preparing the evaluation dataset\n",
    "\n",
    "Prepare set of search results to be evaluated including generation of ground truth using Claude. We will be conducting a series of search using 3 terms from the search_terms_eval list variable. The following cell carries this out by. \n",
    "\n",
    "- Iterate over the search terms list \n",
    "- for each term invoke a semantic search for image + optional text, generated text + optional text, and lastly image + k-NN filter\n",
    "- for each term generate ground truth using Claude (since we do not have real data) \n",
    "- prepare a single variable holding all result sets for calculation of evaluation metrics. This case we are preparing a dictionary object with all results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5381fd29-5d3b-461c-bf13-128296a342ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare set of search results to be evaluated including generation of ground truth\n",
    "\n",
    "test_img_emb = get_titan_multimodal_embedding(image_path=path, dimension=1024)[\"embedding\"]\n",
    "search_results = dict()\n",
    "counter = 1\n",
    "\n",
    "for term in search_terms_eval_list: \n",
    "    \n",
    "    # lexical search \n",
    "    results_lexical = query_text_description(term, 10)\n",
    "\n",
    "    # image + text \n",
    "    query_emb = get_titan_multimodal_embedding(image_path=path, description=term, dimension=1024)[\"embedding\"]\n",
    "    results_semantic_img_txt = query_similarity(query_emb, hits=10, k=2)\n",
    "    \n",
    "    # gen text + text \n",
    "    search_term = res + term\n",
    "    query_emb = get_titan_multimodal_embedding(description=search_term, dimension=1024)[\"embedding\"]\n",
    "    results_semantic_gen_text = query_similarity(query_emb, hits=10, k=2)\n",
    "    \n",
    "    # image + filter\n",
    "    results_semantic_img_filter = query_similarity_filter(test_img_emb,filter_text=term, hits=10, k=2)\n",
    "\n",
    "    # RRF ranking (combine lexical + image + text)\n",
    "    results_rrf = reciprocal_rank_fusion(format_rrf_search_result(results_lexical), format_rrf_search_result(results_semantic_img_txt))\n",
    "    df_results_rrf = pd.DataFrame(results_rrf)\n",
    "\n",
    "    # generate ground truth \n",
    "    formated_system_prompt = prompt.replace(\"{term}\", term).replace(\"{images}\", str(subset_json)).replace(\"{k}\",str(10))\n",
    "    rearranged_list = rearrange_list(bedrock_client, model_id = model_id, prompt=formated_system_prompt, max_tokens=4000, temp=0.1, top_p=0.9)\n",
    "    df_groundtruth = pd.DataFrame(json.loads(rearranged_list))\n",
    "\n",
    "    payload = {\n",
    "        \"lexical\":format_search_result(results_lexical),\n",
    "        \"semantic_img_txt\":format_search_result(results_semantic_img_txt),\n",
    "        \"semantic_gen_txt\":format_search_result(results_semantic_gen_text),\n",
    "        \"semantic_img_filter\":format_search_result(results_semantic_img_filter),\n",
    "        \"rrf\":df_results_rrf,\n",
    "        \"ground_truth\":df_groundtruth\n",
    "    }\n",
    "    \n",
    "    search_results[\"term{}\".format(counter)] = payload\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d62aa5-27db-4c08-800a-0602551a8ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# review the results set quickly\n",
    "df = search_results[\"term3\"][\"ground_truth\"].head()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7edc7d-41dc-41da-8223-a19937651099",
   "metadata": {},
   "source": [
    "### Precision\n",
    "To calculate the precision of a search result, we need to determine the ratio of relevant items in the list to be evaluated compared to the total number of items in that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384ebb6c-1a8d-46a8-a8fd-9bcd9778750d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# precision = count of matched rows / total number of results\n",
    "\n",
    "def calculate_precision(results, results_key=\"semantic_img_txt\"):\n",
    "    precision_results = []\n",
    "    for key, item in results.items():\n",
    "        total_matches = pd.merge(item[\"ground_truth\"], item[results_key], how=\"inner\", on=['image_id']).shape[0]\n",
    "        total_items = item[results_key].shape[0]\n",
    "        precision_results.append(total_matches / total_items)\n",
    "        #print(\"found {0} of matches out of {1}\".format(total_matches, total_items))\n",
    "    return sum(precision_results)/len(precision_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71f7bf0-e65d-492f-b2a4-0cc62d5f7141",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision_img_txt = calculate_precision(search_results,\"semantic_img_txt\")\n",
    "precision_gen_txt = calculate_precision(search_results,\"semantic_gen_txt\")\n",
    "precision_img_filter = calculate_precision(search_results,\"semantic_img_filter\")\n",
    "precision_lexical = calculate_precision(search_results,\"lexical\")\n",
    "precision_rrf = calculate_precision(search_results,\"rrf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bebad03-1198-491b-bc6f-4a5a96c3e690",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Scenario: baseball player image with accompanying terms\")\n",
    "print(\"Precision for lexical: {}%\".format(precision_lexical*100))\n",
    "print(\"Precision for semantic search with Image Embedding and Text Search Term: {}%\".format(precision_img_txt*100))\n",
    "print(\"Precision for semantic search with Generated Text & Text Search Term: {}%\".format(precision_gen_txt*100))\n",
    "print(\"Precision for semantic search with Image Embedding and k-NN Filter: {}%\".format(precision_img_filter*100))\n",
    "print(\"Precision for RRF: {}%\".format(precision_rrf*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed771d2-aa95-454a-90ab-5e0341e4c345",
   "metadata": {},
   "source": [
    "### Mean Reciprocal Rank (MRR)\n",
    "The Mean Reciprocal Rank (MRR) is a metric used to evaluate the performance of a ranking system, such as a search engine or a recommendation system. It measures the average reciprocal rank of the first relevant item in the ranked list.\n",
    "\n",
    "Here's how the MRR algorithm works:\n",
    "\n",
    "1. For each query (or search term), we have a list of relevant items and a list of items to be evaluated (the ranked list returned by the system).\n",
    "2. We iterate through the ranked list and find the position (rank) of the first relevant item.\n",
    "3. If no relevant item is found, the reciprocal rank is considered 0.\n",
    "4. Otherwise, the reciprocal rank is calculated as 1 / rank, where rank is the position of the first relevant item.\n",
    "5. The MRR is the average of the reciprocal ranks across all queries.\n",
    "\n",
    "A higher MRR value indicates better ranking performance, with a maximum value of 1 when the first item in the ranked list is always relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d31813-4a58-499d-9521-252b44f81126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_mrr(results, results_key=\"semantic_img_txt\"):\n",
    "    \n",
    "    total_queries = len(results)\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    for key, item in results.items():\n",
    "        df_ground = item[\"ground_truth\"]\n",
    "        df_results = item[results_key]\n",
    "        matched_rows = pd.merge(df_ground, df_results, how=\"inner\", on=['image_id'])\n",
    "        # position of the first matched row - rank\n",
    "        # if matched_rows contains records then else set to 0\n",
    "        if matched_rows.shape[0] > 0: \n",
    "            rank = df_results[df_results[\"image_id\"] == matched_rows.loc[0,\"image_id\"]].index[0] + 1\n",
    "            reciprocal_ranks.append(1 / rank)\n",
    "        else: \n",
    "            reciprocal_ranks.append(0)\n",
    "\n",
    "    mrr = sum(reciprocal_ranks) / total_queries\n",
    "\n",
    "    return mrr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751429a1-1c03-4a41-9ff3-30cf596c5f8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mrr_lexical = calculate_mrr(search_results,\"lexical\")\n",
    "mrr_img_txt = calculate_mrr(search_results,\"semantic_img_txt\")\n",
    "mrr_gen_txt = calculate_mrr(search_results,\"semantic_gen_txt\")\n",
    "mrr_img_filter = calculate_mrr(search_results,\"semantic_img_filter\")\n",
    "mrr_rrf = calculate_mrr(search_results,\"rrf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee827b1-741b-46ff-928d-7f450bd2d169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Scenario: baseball player image with accompanying terms\")\n",
    "print(\"MRR for lexical search: {}\".format(mrr_lexical))\n",
    "print(\"MRR for semantic search with Image Embedding and Text Search Term: {}\".format(mrr_img_txt))\n",
    "print(\"MRR for semantic search with Generated Text & Text Search Term: {}\".format(mrr_gen_txt))\n",
    "print(\"MRR for semantic search with Image Embedding and k-NN Filter: {}\".format(mrr_img_filter))\n",
    "print(\"MRR for RRF search: {}\".format(mrr_rrf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e06c80a-a6ee-400f-aa91-7796e8b6d8af",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In general we notice that lexical search returns superior results if we enrich the dataset with accurate and extensive metadata. In our case this is evident when we utilise Claude to extract text metadata from a source image and utilise that for embeddings search.\n",
    "\n",
    "However, for relevancy especially when combining multi-modality (image + text) lexical search tends to suffer in both accuracy and relevancy. For this purposes we recommend leveraging Hybrid approaches to boost relevancy. There are several options present to carry this out from using k-NN filters, reciprocal rank fusion to using RAG fusions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a190b11d-a511-4cc1-907d-a56eb924dcdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
